"""Re-Prompting Agent"""

from openai import OpenAI
from openai import AsyncOpenAI
from promptology.config import SERVICE_URL, SERVICE_API_KEY, LLM_MODEL

client = OpenAI(base_url=SERVICE_URL, api_key=SERVICE_API_KEY)
async_client = AsyncOpenAI(base_url=SERVICE_URL, api_key=SERVICE_API_KEY)


SYSTEM_PROMPT = (
    "You are an expert prompt engineer."
    "Think carefully about what makes a prompt effective."
    "They call you 'the promptologist' because you are THE BEST PROMPT ENGINEER EVER."
    "Your prompts are efficient, effective, well formatted."
    "Given: a prompt generated by another AI agent already (<prompt>)"
    "The original user query: <query>"
    "and some user feedback: <feedback>"
    "Your goal: improve the prompt based on the user feedback."
    "Considerations: carefully apply every single piece of feedback to the prompt."
    "Unless user asks to modify the format, keep the same format as the original prompt."
    "Carefully and systematically apply the feedback to the prompt."
    "Only respond with the prompt, nothing else."
    "No explanation is needed, just the prompt."
)


def reprompt(user_query: str, original_prompt: str, feedback: str) -> str:
    """Generate a prompt based on the user's query and all information gathered"""
    response = client.chat.completions.create(
        model=LLM_MODEL,
        messages=[
            {"role": "system", "content": SYSTEM_PROMPT},
            {
                "role": "user",
                "content": f"<user_query>{user_query}</user_query>\n<original_prompt>{original_prompt}</original_prompt>\n<feedback>{feedback}</feedback>",
            },
        ],
    )
    return response.choices[0].message.content


async def reprompt_async(user_query: str, original_prompt: str, feedback: str) -> str:
    """
    Async version of reprompt to be used with FastAPI.

    Args:
        user_query: The original user query
        original_prompt: The current prompt to refine
        feedback: User feedback on the current prompt

    Returns:
        The refined prompt as a string
    """
    response = await async_client.chat.completions.create(
        model=LLM_MODEL,
        messages=[
            {"role": "system", "content": SYSTEM_PROMPT},
            {
                "role": "user",
                "content": f"<user_query>{user_query}</user_query>\n<original_prompt>{original_prompt}</original_prompt>\n<feedback>{feedback}</feedback>",
            },
        ],
    )
    return response.choices[0].message.content
